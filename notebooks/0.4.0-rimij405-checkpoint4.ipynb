{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60462dd1-07f4-46c2-9606-f0f4b4d75dea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint 4 - Multi-Class Classification of Walmart Product Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This checkpoint report summarize's our group's attempts at improving the model performance for the multi-class classification of Walmart product data. In this report, we discuss the performance of our candidate algorithm, the steps taken to tune its performance, and compare the algorithm to a series of other supervised learning models.\n",
    "\n",
    "We explore the following models:\n",
    "\n",
    "1. $k$-Nearest Neighbors\n",
    "1. Logistic Regression\n",
    "1. RBF (Radial Basis-Function) SVC\n",
    "1. Random Forest Classifier (Core Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950b5eb-b762-4c2a-879b-aa50ab942911",
   "metadata": {},
   "source": [
    "As this checkpoint will provide an update over our previous checkpoint's work, the focus of the report has been placed on discussion of the model selection and tuning work. The final presentation will include additional details related to the *formal problem definition*, *key issues*, *related work*, *validation*, *key contributions*, and *future work*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbab89d-0767-4c3f-8ebf-81a6d2b228e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\\Library\\My Repositories\\rit\\2211_FALL\\ISTE780\\Project\n"
     ]
    }
   ],
   "source": [
    "# Enable hot-reloading of external scripts.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set project directory to project root.\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().resolve().parents[0]\n",
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbd2325-1ff2-479e-b76c-77e5f18968da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "# Import utilities.\n",
    "from src.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9838f-b861-43dc-93ae-cf1756ee6f70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "> Can we accurately predict product prices using textual features from the Walmart product dataset?\n",
    "\n",
    "Our group was interested in predicting price ranges based on textual features (eg., `name`, `brand`, `description`, etc.) sourced from a dataset of ~30,000 Walmart product details, scraped by [`PromptCloud.com`](https://www.promptcloud.com/) and [hosted on Kaggle](https://www.kaggle.com/promptcloud/walmart-product-dataset-usa).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Although regression could be used to predict accurate prices, we were interested in the challenges posed by multi-class categorization of text data.\n",
    "\n",
    "Ideally, a small number of explainable price ranges could be used to clearly communicate product price outcomes to an ideal user of our system: a small-business owner interested in identifying products that can be realistically and competitively priced against larger big-brand department stores (eg. Walmart, Amazon, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2dee0-df7e-44c8-a752-c6383be1e52e",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "The original dataset consists of ~30,000 entries representing a sample of products Walmart had listed online in 2019, at the time of `PromptCloud`'s data scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9d3b04-c297-4b55-84bf-27e2a23c735b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('A:/Library/My Repositories/rit/2211_FALL/ISTE780/Project/data/interim/ecommerce_data-cleaned-0.1.4.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29604 entries, 0 to 29999\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   brand         29604 non-null  object \n",
      " 1   name          29604 non-null  object \n",
      " 2   description   29604 non-null  object \n",
      " 3   category_1    29604 non-null  object \n",
      " 4   category_2    29604 non-null  object \n",
      " 5   category_3    29604 non-null  object \n",
      " 6   keywords      29604 non-null  object \n",
      " 7   price_raw     29604 non-null  float64\n",
      " 8   discount_raw  29604 non-null  float64\n",
      " 9   price_range   29604 non-null  object \n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 2.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "products_uri = get_interim_filepath(\"0.1.4\", tag=\"cleaned\")\n",
    "products = pd.read_csv(products_uri, index_col=0, keep_default_na=False)\n",
    "display(products_uri)\n",
    "display(products.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb8206-051b-408d-b34d-0174ce74df15",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26f418-c02b-419b-8298-11e0c66e9cd8",
   "metadata": {},
   "source": [
    "We performed the following preprocessing steps to prepare the data:\n",
    "\n",
    "1. Removed `Walmart`-specific and redundant fields.\n",
    "1. Reorganized and renamed field names for clarity.\n",
    "1. Extracted and engineered new features from `category_raw`:\n",
    "    1. `category_1`, containing the primary category.\n",
    "    1. `category_2`, containing the secondary category.\n",
    "    1. `category_3`, containing the tertiary category.\n",
    "    1. `keywords`, containing category keywords that could not be placed in the previous category features.\n",
    "1. Cleaned textual features in the dataset:\n",
    "    1. Removed unrecognized characters, punctuation.\n",
    "    1. Removed stopwords (eg. \"a\", \"the\", etc.) using the `ntlk` English stop words.\n",
    "    1. Tokenized and stemmed language using a `PorterStemmer` from the `nltk` package.\n",
    "    1. Normalized text to lowercase.\n",
    "1. Extracted and engineered price ranges from the `price_raw` feature. Class labels stored in `price_range`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bf510-ebc9-4e88-ab70-8abe91b9bed9",
   "metadata": {},
   "source": [
    "## TODO: Exploratory Data Analysis\n",
    "\n",
    "We performed exploratory data analysis on the terms in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce038b0b-ccdc-4a82-be51-5c7905142b5a",
   "metadata": {},
   "source": [
    "## Pipeline Setup\n",
    "\n",
    "The `sklearn` library provides extensive support for data science pipelines through the use of the `Pipeline`, `FeatureUnion`, and `ColumnTransformer` pipeline composition tools. We use these utilities (among others) in order to create our classifier models, measure performance, and report scores. This section describes our preparation work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3efb3-303b-4e27-be56-e853317c0efc",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We use a majority of the features present in the preprocessed dataset we import. The following step will exclude the response from the dataset and drop two unused columns: `price_raw` and `discount_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb43ff2-40f7-438b-875c-50ae9fc4d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29604 entries, 0 to 29999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   brand        29604 non-null  object\n",
      " 1   name         29604 non-null  object\n",
      " 2   description  29604 non-null  object\n",
      " 3   category_1   29604 non-null  object\n",
      " 4   category_2   29604 non-null  object\n",
      " 5   category_3   29604 non-null  object\n",
      " 6   keywords     29604 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Create list with features to use.\n",
    "features = [ \n",
    "    'brand', 'name', 'description', \n",
    "    'category_1', 'category_2', 'category_3',\n",
    "    'keywords']\n",
    "\n",
    "# Select feature columns only.\n",
    "X = products.loc[:, features]\n",
    "\n",
    "# Display the information about the features dataframe.\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a91e7-343e-46f1-ade8-ed8738722ca6",
   "metadata": {},
   "source": [
    "### Response Encoding\n",
    "\n",
    "In order to utilize `sklearn`'s classifiers, we encoded the categorical response variable `price_range` using the `LabelEncoder` preprocessing utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5551f70c-d2be-4e4b-a252-3b0682b00e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29604 entries, 0 to 29603\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   y       29604 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 115.8 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(25, 50]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 25]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(100, 100+]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(50, 100]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label  Class\n",
       "0     (25, 50]      0\n",
       "1      (0, 25]      1\n",
       "2  (100, 100+]      2\n",
       "3    (50, 100]      3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels \n",
    "label_encoder = LabelEncoder()\n",
    "labels = products.loc[:,\"price_range\"]\n",
    "y = label_encoder.fit_transform(labels)\n",
    "display(pd.DataFrame({'y': y}).info())\n",
    "\n",
    "# Display the unique labels and codes.\n",
    "pd.DataFrame({'Label': labels.unique(), 'Class': np.unique(y)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91989688-de86-429b-9e5e-2666323be92f",
   "metadata": {},
   "source": [
    "### Subset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03279dc2-f9e0-486c-8369-e809f59a4dc7",
   "metadata": {},
   "source": [
    "In order to estimate how our models will perform on new, previously unseen data, we fit our models on a training subset and test them on a held-out validation subset. Due to the imbalanced distribution of price ranges by category class, we ensure that our splits are stratified. The `train_test_split` function provided by the `sklearn.model_selection` package allows us to split our data while respecting the distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "553a3ebf-cd26-404c-b12f-21eb018bebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare split percentages.\n",
    "pct_train = 0.20\n",
    "pct_test = 1 - pct_train\n",
    "\n",
    "# Create a train-test split, samples stratified by class.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = pct_test, random_state = 20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b62eec4f-6b33-4ed3-a499-9d72ee136d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X train: (5920, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5 largest category value counts in training set: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sport outdoor    2188\n",
       "food              809\n",
       "health            755\n",
       "babi              562\n",
       "person care       467\n",
       "Name: category_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y train: (5920,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Breakdown of training set classes: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class (train)\n",
       "0                3333\n",
       "2                1248\n",
       "3                 683\n",
       "1                 656\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display details about each train split.\n",
    "display(f\"X train: {X_train.shape}\")\n",
    "display(\"5 largest category value counts in training set: \")\n",
    "display(X_train['category_1'].value_counts().nlargest(5))\n",
    "display(f\"y train: {y_train.shape}\")\n",
    "display(\"Breakdown of training set classes: \")\n",
    "display(pd.DataFrame({'Class (train)': y_train}).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "822d4cba-7011-4213-8262-9c1cdd8f7845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X test: (23684, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5 largest category value counts in testing set: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sport outdoor    8775\n",
       "health           3152\n",
       "food             3128\n",
       "babi             2147\n",
       "person care      1836\n",
       "Name: category_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y test: (23684,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Breakdown of testing set classes: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class (test)\n",
       "0               13337\n",
       "2                4991\n",
       "3                2733\n",
       "1                2623\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display details about each test split.\n",
    "display(f\"X test: {X_test.shape}\")\n",
    "display(\"5 largest category value counts in testing set: \")\n",
    "display(X_test['category_1'].value_counts().nlargest(5))\n",
    "display(f\"y test: {y_test.shape}\")\n",
    "display(\"Breakdown of testing set classes: \")\n",
    "display(pd.DataFrame({'Class (test)': y_test}).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181cbb4-67b6-48bf-b4ec-6f0cf14a279d",
   "metadata": {},
   "source": [
    "### Pipeline Composition\n",
    "\n",
    "The `Pipeline` concept allows us to sequentially apply transformers to our input dataset prior to then using the transformed data on a classifier of our choice. In\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d9de1-7f00-4f49-8292-78bdc303b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the pipeline.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", sublinear_tf=True, norm='l2')\n",
    "column_transformer = ColumnTransformer([('name', vectorizer, 'name'),\n",
    "                                        ('description', vectorizer, 'description'),\n",
    "                                        ('brand', vectorizer, 'brand'),\n",
    "                                        ('category_raw', vectorizer, 'category_raw'),\n",
    "                                       ], remainder='drop', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b973b-ad2c-4a23-9e61-a6071d37e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation function:\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def show_metrics(clf, test_X, test_y):\n",
    "    print(f'Classification score: {clf.score(test_X, test_y) * 100}%')\n",
    "    print(classification_report(np.array(test_y), clf.predict(test_X), zero_division=0))\n",
    "    print(confusion_matrix(np.array(test_y), clf.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7519ea2-a968-4d50-b2f0-16186e265d67",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "\n",
    "In order to compare our models to a reasonable baseline, we fit the model features using a `DummyClassifier` that makes predictions using simple rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84a9b9-6f88-43d7-aee2-e28e21096071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DummyClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create the DummyClassifier pipeline.\n",
    "clf_dummy = Pipeline([('vect', column_transformer),\n",
    "                      ('chi', SelectKBest(chi2, k=7000)),\n",
    "                      ('clf', DummyClassifier(strategy='most_frequent'))])\n",
    "\n",
    "# Fit the dummy classifier.\n",
    "clf_dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f31d88-b89e-4a3d-b635-77a4d50fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(clf_dummy, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605239f6-2485-4e0e-83ba-6ad724872a54",
   "metadata": {},
   "source": [
    "The dummy classifier serves as a useful baseline: something to compare our models' performance against. In this instance, it selects the most frequent class in the label distribution and achieves a classification score of roughly $\\approx 26.35$%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ee804-fbd6-4be1-bebb-8e67e970dad0",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbor Classifier\n",
    "\n",
    "K-Nearest Neighbor (KNN) is a non-parametric classification algorithm that tries to classify a given observation to a response class with the highest estimated probability. For a given positive value of K, the classifier identifies K points from the training data set that are closest to the test observation (i.e. it’s K nearest neighbors). Then it computes the estimated conditional probability using the Bayes rule and classifies the test observation to the response class with the largest probability. In our project, KNN can be used to model the List Price of a Walmart product by finding the K-nearest neighbors and assigning the list price label that has the highest estimated probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635af976-18eb-454a-9a90-fe81d2ee52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the baseline kNN pipeline\n",
    "clf_kNN = Pipeline([('vect', column_transformer),\n",
    "                      ('chi',  SelectKBest(chi2, k=7000)),\n",
    "                      ('clf', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0479a-ff0a-4745-9724-c137532cd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the kNN classifier.\n",
    "clf_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a127f-6356-4d1c-b50e-f192faf7f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(clf_kNN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4014f-2042-4b8b-a62e-12c527945e2e",
   "metadata": {},
   "source": [
    "### Tuning the $k$-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f5ebe-8e8d-4900-94a1-4eabe8eda298",
   "metadata": {},
   "source": [
    "We attempted to use the elbow method to calculate an optimal $k$ for our $k$-Nearest Neighbor classifier. We narrowed it down to a range between $[5, 12]$ on a smaller sample of ~2000 before applying our algorithm to the entire ~20,000+ records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4123c-5f10-4dec-a9c1-4a221033effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using the elbow method to find optimal K.\n",
    "error_rate = []\n",
    "\n",
    "tuple_range =range(5,12)\n",
    "# Will take some time\n",
    "for i in tuple_range:\n",
    "    elb_KNN = Pipeline([('vect', column_transformer),\n",
    "                      ('chi',  SelectKBest(chi2, k=7000)),\n",
    "                      ('clf', KNeighborsClassifier(n_neighbors=i))])\n",
    "    elb_KNN.fit(X_train, y_train)\n",
    "    y_i = elb_KNN.predict(X_test)\n",
    "    error_rate.append(np.mean(y_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049affe8-421c-4b1a-9294-4ba45550d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(tuple_range,error_rate,color='blue', linestyle='dashed', marker='o',\n",
    " markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a0aea-4a20-499c-afde-b2f450cff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup GridSearchCV for our model.\n",
    "parameters = {'n_neighbors':range(5,10)}\n",
    "knn = KNeighborsClassifier()\n",
    "cv_KNN = GridSearchCV(knn, parameters)\n",
    "\n",
    "# Create the baseline kNN pipeline\n",
    "clf_kNN2 = Pipeline([('vect', column_transformer),\n",
    "                      ('chi',  SelectKBest(chi2, k=7000)),\n",
    "                      ('clf', cv_KNN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548dae9-9d28-4474-b791-33b833cfc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the kNN classifier.\n",
    "clf_kNN2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32a905-ae66-4908-a65c-d8cdc949bea8",
   "metadata": {},
   "source": [
    "Unfortunately, our grid search for hyperparameter tuning did not yield noticeable change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5f8de-2129-47e7-9aa4-48d06cce685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(clf_kNN2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3251131-84ab-4109-af44-d3eb55fb5596",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical model that can be used to model the probability that the response Y belongs to a particular category/class. This is different from other classification algorithms that model the response Y directly. In our project, Logistic Regression can be used to model the probability that the List Price of a Walmart product belongs to any of the labels. Logistic Regression uses a logistic function to model a statistically dependent variable (typically binary). In a binary logistic regression problem, the dependent variable (i.e., the response Y) can have two possible categorical values such as “0” and “1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71af7f8-5e1b-4e15-a877-84bacc235d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the pipeline.\n",
    "clf_logreg = Pipeline([('vect', column_transformer),\n",
    "                      ('chi', SelectKBest(chi2, k=7000)),\n",
    "                      ('clf', LogisticRegression(multi_class='multinomial', max_iter=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27eb1c4-be90-42be-97c7-d8063d4b9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier.\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "show_metrics(clf_logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e3a69-f455-4e91-9ec6-310cc93dd046",
   "metadata": {},
   "source": [
    "Logistic regression performs much better than the dummy classifier, with a $42.67$% classification score. We could choose this model to further tune, changing the decision boundary probability to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ef884-3b3a-4d72-80bb-1e235258e9bc",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "The random forest classifier is an ensemble estimator that fits a series of decision trees on various sub-samples of the dataset. `sklearn`'s implementation uses bootstrapping by default and uses the `gini` index as a measure of node purity in each of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a514ba-cd4c-414c-9b98-0ff531629faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the pipeline.\n",
    "clf_RF = Pipeline([('vect', column_transformer),\n",
    "                   ('chi', SelectKBest(chi2, k=7000)),\n",
    "                   ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317c076-a55e-44e0-8243-14dda032cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier.\n",
    "clf_RF.fit(X_train, y_train)\n",
    "show_metrics(clf_RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71465e-38fc-4f47-a6fc-a4aaa723be7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RBF (Radial Basis Function) SVC\n",
    "\n",
    "SVC stands for C-Support Vector Classification. According to skcikit learn, \"The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples.\" SVC is using a radial basis function for its kernel to build a \"one vs one\" model. \n",
    "\n",
    "Support Vector Machines (SVMs) are used for solving supervised learning classification problems, but they can also be used for clustering and regression algorithms. SVM tries to find a hyperplane that separates the response classes with highest margin possible. The points that lie on the margins are called support vectors. SVM uses a kernel called radial basis function to build a one vs one model for the prediction with approximately 43% accuracy. RBF is the default kernel used within scikit-learn’s SVM algorithm, and it helps to control individual observation’s effect on the overall algorithm. Large values of gamma parameter indicate greater effect of test observation on the overall algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31d585-8e9f-47e3-b339-7dbcb2acb391",
   "metadata": {},
   "source": [
    "### Baseline SVC (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277dc8d-6965-4fc1-b864-cca78e11b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the pipeline.\n",
    "clf_SVC = Pipeline([('vect', column_transformer),\n",
    "                   ('chi',  SelectKBest(chi2, k=7000)),\n",
    "                   ('clf', SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6191930-8f38-42f3-aac1-f669be7005f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier.\n",
    "clf_SVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c27ab3-95b8-42ae-9f63-1d0337b2fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(clf_SVC, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26be6f-5f3a-4c30-a351-78441b9e51c7",
   "metadata": {},
   "source": [
    "We performed a cross-validation measurement of SVC on a small subset of ~2000 samples but it did not improve classification performance, so we elected not to run the full dataset on the cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5aac4-805f-45c7-9476-74559e525865",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Fitting information from roughly 30,000 products is a computationally intensive process. One of the challenges we can address is fitting models on a smaller sub-sample of the data in such a way that our findings extrapolate well once we increase the amount of samples used. Initially, we setup our models using ~2000 samples from the larger population.\n",
    "\n",
    "Considering that the `DummyClassifier` has a classification score of ~26%, there is a clear improvement to the process that comes from using the other models. Hyperparameter tuning can be used to improve the performance of the different models.\n",
    "\n",
    "It is possible that we could redefine the classification we're trying to ask. Instead of the challenging multi-class classification, the problem domain could be reduced. Exploring a smaller number of labels or even turning the problem into a binary classification tasks may work well, especially in terms of something like the logistic regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
