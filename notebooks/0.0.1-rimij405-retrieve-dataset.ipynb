{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.1 - Retrieve & Clean Dataset\n",
    "\n",
    "**Overview**: This notebook demonstrates how to download and extract the Kaggle dataset using the Kaggle API python package and the `doit` task automation tool.\n",
    "\n",
    "**Actions**: This notebook executes tasks using the `doit` task automation tool in order to:\n",
    "\n",
    "- Downloads the Kaggle dataset using your Kaggle API credentials (download_dataset.py).\n",
    "- Extracts the appropriate `.csv` file from the `.zip` source (unpack_dataset.py).\n",
    "\n",
    "**Dependencies**: This notebook has the following dependencies:\n",
    "\n",
    "- Users should have installed the dependencies listed in `environment.yaml` or `requirements.txt` using the appropriate tool.\n",
    "- Users should also have their Kaggle credentials appropriately set using one of two options:\n",
    "    - A valid `kaggle.json` file in their home configuration folder (`~/.kaggle/kaggle.json`) or,\n",
    "    - Appropriately set `KAGGLE_USERNAME` and `KAGGLE_KEY` environment variables in a `.env` file that is excluded from version control.\n",
    "\n",
    "**Targets**: This notebook outputs three artifacts:\n",
    "\n",
    "- `data/raw/walmart-product-data-2019.zip`\n",
    "- `data/raw/marketing_sample_for_walmart_com-ecommerce__20191201_20191231__30k_data.csv`.\n",
    "- `data/interim/ecommerce_data-cleaned-0.1.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cell changes into the root project directory in order to execute command line tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repositories\\rit\\ISTE780\\Project\n"
     ]
    }
   ],
   "source": [
    "# Change current working directory to project root.\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().resolve().parents[0]\n",
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_data        Clean the raw dataset (.csv) and place in the interim folder.\n",
      "create_data_dir   Create the data directory.\n",
      "create_env        Create new conda environment using 'environment.yaml'.\n",
      "download_data     Download the dataset from Kaggle.\n",
      "unpack_data       Unpack the raw dataset (.zip) as the (.csv) file.\n",
      "update_env        Update existing conda environment using 'environment.yaml'.\n"
     ]
    }
   ],
   "source": [
    "# List the possible tasks.\n",
    "!doit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- create_data_dir\n",
      "-- create_env\n",
      "-- download_data\n",
      "-- unpack_data\n",
      "-- clean_data\n"
     ]
    }
   ],
   "source": [
    "# Performs the initial cleaning of the data.\n",
    "!doit clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\interim\\ecommerce_data-cleaned-0.1.csv\n",
      "data\\interim.bak\\.ipynb_checkpoints\\ecommerce_data-cleaned-0.1-checkpoint.csv\n",
      "data\\interim.bak\\.ipynb_checkpoints\\ecommerce_data-cleaned-0.2.2-checkpoint.csv\n",
      "data\\interim.bak\\.ipynb_checkpoints\\ecommerce_data-cleaned-0.2.3-checkpoint.csv\n",
      "data\\interim.bak\\ecommerce_data-cleaned-0.1.0.csv\n",
      "data\\interim.bak\\ecommerce_data-cleaned-0.1.csv\n",
      "data\\interim.bak\\ecommerce_data-cleaned-0.2.1.csv\n",
      "data\\interim.bak\\ecommerce_data-cleaned-0.2.2.csv\n",
      "data\\interim.bak\\ecommerce_data-cleaned-0.2.3.csv\n",
      "data\\raw\\marketing_sample_for_walmart_com-ecommerce__20191201_20191231__30k_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Display files in the data folders.\n",
    "import os\n",
    "for root, dir, files in os.walk(\"data/\", topdown=False):\n",
    "    for name in files:\n",
    "        if \".csv\" in name:\n",
    "            print(Path(root) / name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b946e3ad701e39b274e0926621a4c5b53e2fa16d17112201a160d3918c4e637"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
