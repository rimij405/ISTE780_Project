{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import regex\n",
    "\n",
    "data_uri = '../data/interim/ecommerce_data-cleaned-0.1.4.csv'\n",
    "\n",
    "products_raw = pd.read_csv(data_uri, index_col=0, keep_default_na=False)\n",
    "products_raw.head()\n",
    "\n",
    "N = 1500\n",
    "products_raw = products_raw.groupby('price_range', group_keys=False).apply(lambda x: x.sample(int(np.rint(N*len(x)/len(products_raw))))).sample(frac=1).reset_index(drop=True)\n",
    "products_raw.head()\n",
    "\n",
    "# Prepare the train/test splits.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_range_label(price):\n",
    "    value = np.round(price, decimals=1)\n",
    "    if value <= 25:\n",
    "        return 0\n",
    "    elif 25 < value <= 50:\n",
    "        return 1\n",
    "    elif 50 < value <= 100:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "products_raw['labels'] = products_raw['price_raw'].apply(lambda x: get_range_label(x))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "cleaned_text = lambda x: \" \".join([stemmer.stem(i) for i in regex.sub(\"[^a-zA-Z0-9]\", \" \", x).split() if i not in words]).lower()\n",
    "products_raw['cleaned_name'] = products_raw.name.apply(cleaned_text)\n",
    "products_raw['cleaned_brand'] = products_raw.brand.fillna(\"\").apply(cleaned_text)\n",
    "products_raw['cleaned_description'] = products_raw.description.fillna(\"\").apply(cleaned_text)\n",
    "products_raw['cleaned_category_1'] = products_raw.category_1.fillna(\"\").apply(cleaned_text)\n",
    "products_raw['cleaned_category_2'] = products_raw.category_2.fillna(\"\").apply(cleaned_text)\n",
    "products_raw['cleaned_category_3'] = products_raw.category_3.fillna(\"\").apply(cleaned_text)\n",
    "products_raw['cleaned_keywords'] = products_raw.keywords.fillna(\"\").apply(cleaned_text)\n",
    "# display(products_raw)\n",
    "\n",
    "X = products_raw\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, products_raw['labels'], test_size=0.25)\n",
    "# X = products_raw.drop(columns=['price_raw', 'discount_raw'])\n",
    "# y = products_raw['price_raw'].astype('int')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.57      1.00      0.73       214\n           1       0.00      0.00      0.00        75\n           2       0.00      0.00      0.00        42\n           3       0.00      0.00      0.00        44\n\n    accuracy                           0.57       375\n   macro avg       0.14      0.25      0.18       375\nweighted avg       0.33      0.57      0.41       375\n\n[[214   0   0   0]\n [ 75   0   0   0]\n [ 42   0   0   0]\n [ 44   0   0   0]]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/Users/sheenambhatia/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/sheenambhatia/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/sheenambhatia/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Prepare the pipeline.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, DictionaryLearning, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def get_feature_transformer(columns, vectorizer):\n",
    "    return ColumnTransformer([(feature, vectorizer, feature) for feature in columns], remainder='drop', verbose_feature_names_out=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "\n",
    "# column_transformer = get_feature_transformer(['brand', 'name', 'description', 'category_1', 'category_2', 'category_3', 'keywords'], vectorizer)\n",
    "\n",
    "column_transformer = ColumnTransformer([('name', vectorizer, 'cleaned_name'),('description', vectorizer, 'cleaned_description'), \n",
    "                                  ('brand', vectorizer, 'cleaned_brand'), ('category_1', vectorizer, 'cleaned_category_1'),\n",
    "                                  ('category_2', vectorizer, 'cleaned_category_2'), ('category_3', vectorizer, 'cleaned_category_3'), \n",
    "                                  ('keywords', vectorizer, 'cleaned_keywords')],\n",
    "                                remainder='drop', verbose_feature_names_out=False)\n",
    "\n",
    "def show_metrics(clf, test_X, test_y):\n",
    "    print(f'Classification score: {clf.score(test_X, test_y) * 100}%')\n",
    "    print(classification_report(np.array(test_y), clf.predict(test_X), zero_division=0))\n",
    "    print(confusion_matrix(np.array(test_y), clf.predict(test_X)))\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_pipeline():\n",
    "    \"\"\"Get the composed Pipeline\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"vect\", column_transformer),\n",
    "        (\"dim\", \"passthrough\"),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "N_FEATURES = [2, 4, 10]\n",
    "\n",
    "def get_param_grid():\n",
    "    return [\n",
    "        {\n",
    "            \"dim\": [TruncatedSVD(), LatentDirichletAllocation()],\n",
    "            \"dim__n_components\": N_FEATURES,\n",
    "        },\n",
    "        {\n",
    "            \"dim\": [SelectKBest(chi2, k=7000)],\n",
    "            \"dim__k\": N_FEATURES,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "reducer_labels = [\"TruncatedSVD\", \"LDA\", \"KBest(chi2)\"]\n",
    "grid = GridSearchCV(get_pipeline(), n_jobs=1, param_grid=get_param_grid())\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# show_metrics(grid, X_test, y_test)\n",
    "\n",
    "ytest_RF = np.array(y_test)\n",
    "\n",
    "print(classification_report(ytest_RF, grid.predict(X_test)))\n",
    "print(confusion_matrix(ytest_RF, grid.predict(X_test)))\n",
    "\n",
    "\n",
    "# clf_RF = Pipeline([('vect', column_transformer),\n",
    "#                    ('chi',  SelectKBest(chi2, k=7000)),\n",
    "#                    ('clf', RandomForestClassifier())])\n",
    "# # Fit the classifier.\n",
    "# clf_RF = clf_RF.fit(X_train, y_train)\n",
    "\n",
    "# ytest_RF = np.array(y_test)\n",
    "# \n",
    "# print(classification_report(ytest_RF, clf_RF.predict(X_test)))\n",
    "# print(confusion_matrix(ytest_RF, clf_RF.predict(X_test)))\n",
    "\n",
    "# show_metrics(clf_RF, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\nKeyboardInterrupt\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# column_transformer = ColumnTransformer([('name', vectorizer, 'cleaned_name'),('description', vectorizer, 'cleaned_description'), \n",
    "#                                   ('brand', vectorizer, 'cleaned_brand'), ('category_1', vectorizer, 'cleaned_category_1'),\n",
    "#                                   ('category_2', vectorizer, 'cleaned_category_2'), ('category_3', vectorizer, 'cleaned_category_3'), \n",
    "#                                   ('keywords', vectorizer, 'cleaned_keywords')],\n",
    "#                                 remainder='drop', verbose_feature_names_out=False)\n",
    "# \n",
    "# \n",
    "# \n",
    "# clf_LDA = Pipeline([('vect', column_transformer),\n",
    "#                     ('svd', LatentDirichletAllocation()),\n",
    "#                    ('clf', RandomForestClassifier())])\n",
    "# # Fit the classifier.\n",
    "# clf_LDA = clf_LDA.fit(X_train, y_train)\n",
    "# # show_metrics(clf_LDA, X_test, y_test)\n",
    "# ytest_RF = np.array(y_test)\n",
    "\n",
    "# print(classification_report(ytest_RF, clf_LDA.predict(X_test)))\n",
    "# print(confusion_matrix(ytest_RF, clf_LDA.predict(X_test)))\n",
    "\n",
    "# clf_DL = Pipeline([('vect', column_transformer),\n",
    "#                     ('svd', DictionaryLearning()),\n",
    "#                    ('clf', RandomForestClassifier())])\n",
    "# # Fit the classifier.\n",
    "# clf_DL.fit(X_train, y_train)\n",
    "# show_metrics(clf_DL, X_test, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Classification score: 4.485880286447777%\n",
      "              precision    recall  f1-score   support\n\n           0       0.12      0.17      0.14         6\n           1       0.17      0.07      0.10        29\n           2       0.03      0.05      0.04        44\n           3       0.07      0.07      0.07        95\n           4       0.03      0.03      0.03       141\n           5       0.06      0.07      0.06       215\n           6       0.08      0.10      0.09       256\n           7       0.08      0.10      0.09       268\n           8       0.07      0.08      0.07       270\n           9       0.04      0.05      0.05       282\n          10       0.05      0.05      0.05       191\n          11       0.05      0.05      0.05       234\n          12       0.09      0.08      0.08       246\n          13       0.06      0.06      0.06       202\n          14       0.08      0.07      0.08       240\n          15       0.06      0.06      0.06       165\n          16       0.01      0.01      0.01       152\n          17       0.02      0.03      0.02       154\n          18       0.03      0.03      0.03       133\n          19       0.06      0.07      0.07       218\n          20       0.01      0.01      0.01       111\n          21       0.06      0.05      0.05       128\n          22       0.04      0.04      0.04       126\n          23       0.03      0.02      0.02       105\n          24       0.06      0.06      0.06       159\n          25       0.04      0.03      0.03       114\n          26       0.03      0.02      0.03        83\n          27       0.04      0.03      0.04        87\n          28       0.04      0.04      0.04        77\n          29       0.04      0.04      0.04       136\n          30       0.05      0.05      0.05        56\n          31       0.03      0.03      0.03        73\n          32       0.02      0.02      0.02        57\n          33       0.09      0.09      0.09        58\n          34       0.01      0.01      0.01        82\n          35       0.02      0.02      0.02        56\n          36       0.00      0.00      0.00        51\n          37       0.12      0.12      0.12        49\n          38       0.02      0.02      0.02        66\n          39       0.02      0.02      0.02        92\n          40       0.03      0.03      0.03        39\n          41       0.03      0.03      0.03        37\n          42       0.12      0.12      0.12        50\n          43       0.03      0.03      0.03        35\n          44       0.00      0.00      0.00        50\n          45       0.00      0.00      0.00        35\n          46       0.10      0.06      0.07        49\n          47       0.00      0.00      0.00        34\n          48       0.00      0.00      0.00        28\n          49       0.02      0.03      0.03        69\n          50       0.00      0.00      0.00        27\n          51       0.00      0.00      0.00        29\n          52       0.00      0.00      0.00        23\n          53       0.00      0.00      0.00        19\n          54       0.00      0.00      0.00        30\n          55       0.00      0.00      0.00        23\n          56       0.00      0.00      0.00        24\n          57       0.00      0.00      0.00        20\n          58       0.00      0.00      0.00        22\n          59       0.03      0.04      0.04        52\n          60       0.06      0.12      0.08         8\n          61       0.00      0.00      0.00        11\n          62       0.00      0.00      0.00        13\n          63       0.00      0.00      0.00        24\n          64       0.06      0.04      0.05        27\n          65       0.22      0.12      0.15        17\n          66       0.00      0.00      0.00        10\n          67       0.00      0.00      0.00        10\n          68       0.00      0.00      0.00        13\n          69       0.02      0.02      0.02        44\n          70       0.00      0.00      0.00        17\n          71       0.00      0.00      0.00        11\n          72       0.00      0.00      0.00        12\n          73       0.00      0.00      0.00        13\n          74       0.00      0.00      0.00        19\n          75       0.09      0.10      0.10        10\n          76       0.07      0.06      0.06        17\n          77       0.00      0.00      0.00        10\n          78       0.00      0.00      0.00        12\n          79       0.04      0.05      0.05        40\n          80       0.17      0.17      0.17        12\n          81       0.00      0.00      0.00         6\n          82       0.00      0.00      0.00        11\n          83       0.00      0.00      0.00        11\n          84       0.00      0.00      0.00        19\n          85       0.00      0.00      0.00        10\n          86       0.00      0.00      0.00         6\n          87       0.00      0.00      0.00        10\n          88       0.00      0.00      0.00        11\n          89       0.00      0.00      0.00        32\n          90       0.00      0.00      0.00        10\n          91       0.00      0.00      0.00         6\n          92       0.00      0.00      0.00         9\n          93       0.00      0.00      0.00         8\n          94       0.00      0.00      0.00        13\n          95       0.00      0.00      0.00         3\n          96       0.00      0.00      0.00         6\n          97       0.00      0.00      0.00         8\n          98       0.12      0.09      0.11        11\n          99       0.00      0.00      0.00        36\n         100       0.00      0.00      0.00        10\n         101       0.00      0.00      0.00         3\n         102       0.00      0.00      0.00         3\n         103       0.00      0.00      0.00         6\n         104       0.00      0.00      0.00         8\n         105       0.00      0.00      0.00         4\n         106       0.00      0.00      0.00         5\n         107       0.00      0.00      0.00         6\n         108       0.00      0.00      0.00         4\n         109       0.08      0.06      0.07        16\n         110       0.00      0.00      0.00         6\n         111       0.00      0.00      0.00         1\n         112       0.00      0.00      0.00         4\n         113       0.00      0.00      0.00         7\n         114       0.00      0.00      0.00         3\n         115       0.00      0.00      0.00         8\n         116       0.20      0.20      0.20         5\n         117       0.00      0.00      0.00         8\n         118       0.00      0.00      0.00         4\n         119       0.00      0.00      0.00        14\n         120       0.00      0.00      0.00         0\n         121       0.00      0.00      0.00         3\n         122       0.00      0.00      0.00         3\n         123       0.00      0.00      0.00         2\n         124       0.00      0.00      0.00         9\n         125       0.33      0.33      0.33         3\n         126       0.00      0.00      0.00         4\n         127       0.00      0.00      0.00         3\n         128       0.00      0.00      0.00         9\n         129       0.03      0.04      0.04        24\n         130       0.00      0.00      0.00         5\n         131       0.00      0.00      0.00         6\n         132       0.00      0.00      0.00         7\n         133       0.00      0.00      0.00         1\n         134       0.00      0.00      0.00         5\n         135       0.00      0.00      0.00         3\n         136       0.00      0.00      0.00         2\n         137       0.00      0.00      0.00         2\n         138       0.00      0.00      0.00         7\n         139       0.00      0.00      0.00        18\n         140       0.00      0.00      0.00         2\n         141       0.00      0.00      0.00         2\n         142       0.00      0.00      0.00         5\n         143       0.00      0.00      0.00         4\n         144       0.00      0.00      0.00         0\n         145       0.00      0.00      0.00         8\n         146       0.00      0.00      0.00         8\n         147       0.00      0.00      0.00         2\n         148       0.00      0.00      0.00         4\n         149       0.05      0.07      0.06        15\n         150       0.00      0.00      0.00         3\n         151       0.00      0.00      0.00         3\n         152       0.00      0.00      0.00         2\n         153       0.00      0.00      0.00         3\n         154       0.00      0.00      0.00         1\n         155       0.00      0.00      0.00         2\n         156       0.00      0.00      0.00         2\n         157       0.00      0.00      0.00         2\n         158       0.00      0.00      0.00         3\n         159       0.00      0.00      0.00        19\n         160       0.00      0.00      0.00         2\n         161       0.00      0.00      0.00         3\n         162       0.00      0.00      0.00         2\n         163       0.00      0.00      0.00         3\n         164       0.00      0.00      0.00         2\n         165       0.00      0.00      0.00         3\n         166       0.00      0.00      0.00         1\n         167       0.00      0.00      0.00         3\n         168       0.00      0.00      0.00         2\n         169       0.00      0.00      0.00        11\n         170       0.00      0.00      0.00         3\n         171       0.00      0.00      0.00         1\n         172       0.00      0.00      0.00         1\n         173       0.00      0.00      0.00         0\n         174       0.17      0.17      0.17         6\n         175       0.00      0.00      0.00         1\n         176       0.00      0.00      0.00         1\n         178       0.00      0.00      0.00         1\n         179       0.00      0.00      0.00        23\n         180       0.00      0.00      0.00         1\n         181       0.00      0.00      0.00         5\n         182       0.00      0.00      0.00         4\n         183       0.00      0.00      0.00         0\n         184       0.00      0.00      0.00         3\n         185       0.00      0.00      0.00         4\n         186       0.00      0.00      0.00         0\n         187       0.00      0.00      0.00         0\n         188       0.00      0.00      0.00         2\n         189       0.00      0.00      0.00         7\n         190       0.00      0.00      0.00         1\n         191       0.00      0.00      0.00         5\n         192       0.00      0.00      0.00         1\n         193       0.00      0.00      0.00         6\n         194       0.00      0.00      0.00         2\n         195       0.00      0.00      0.00         0\n         196       0.00      0.00      0.00         1\n         197       0.00      0.00      0.00         2\n         198       0.00      0.00      0.00         1\n         199       0.00      0.00      0.00        20\n         200       0.00      0.00      0.00         2\n         202       0.00      0.00      0.00         1\n         204       1.00      0.38      0.55         8\n         206       0.00      0.00      0.00         1\n         207       0.00      0.00      0.00         0\n         209       0.25      0.11      0.15         9\n         210       0.00      0.00      0.00         1\n         211       0.00      0.00      0.00         2\n         213       0.00      0.00      0.00         1\n         214       0.00      0.00      0.00         1\n         215       0.00      0.00      0.00         2\n         216       1.00      0.50      0.67         2\n         217       1.00      0.50      0.67         2\n         219       0.00      0.00      0.00         9\n         220       0.00      0.00      0.00         2\n         221       0.00      0.00      0.00         3\n         223       0.00      0.00      0.00         1\n         224       0.00      0.00      0.00         4\n         225       0.00      0.00      0.00         0\n         229       0.00      0.00      0.00         3\n         230       0.40      0.67      0.50         3\n         233       0.00      0.00      0.00         1\n         234       0.00      0.00      0.00         2\n         235       0.00      0.00      0.00         3\n         236       0.00      0.00      0.00         0\n         237       0.00      0.00      0.00         1\n         238       0.00      0.00      0.00         0\n         239       0.00      0.00      0.00         2\n         240       0.00      0.00      0.00         1\n         241       1.00      1.00      1.00         1\n         242       0.00      0.00      0.00         1\n         243       0.00      0.00      0.00         0\n         244       0.00      0.00      0.00         1\n         245       0.00      0.00      0.00         0\n         246       0.00      0.00      0.00         2\n         247       0.00      0.00      0.00         1\n         249       0.00      0.00      0.00         9\n         250       0.00      0.00      0.00         1\n         251       0.00      0.00      0.00         1\n         256       0.00      0.00      0.00         1\n         258       0.00      0.00      0.00         1\n         259       0.00      0.00      0.00         2\n         260       0.00      0.00      0.00         0\n         262       0.00      0.00      0.00         2\n         263       0.00      0.00      0.00         0\n         264       0.00      0.00      0.00         1\n         265       0.00      0.00      0.00         0\n         267       0.00      0.00      0.00         1\n         268       0.00      0.00      0.00         0\n         269       0.00      0.00      0.00         7\n         270       0.00      0.00      0.00         2\n         271       0.00      0.00      0.00         2\n         272       0.00      0.00      0.00         1\n         273       0.00      0.00      0.00         1\n         274       0.00      0.00      0.00         0\n         278       0.00      0.00      0.00         3\n         279       0.00      0.00      0.00         5\n         280       0.00      0.00      0.00         1\n         281       0.00      0.00      0.00         1\n         283       0.00      0.00      0.00         1\n         284       0.00      0.00      0.00         1\n         287       0.00      0.00      0.00         2\n         288       0.00      0.00      0.00         2\n         289       0.00      0.00      0.00         5\n         290       0.00      0.00      0.00         2\n         291       0.00      0.00      0.00         0\n         292       0.00      0.00      0.00         1\n         293       0.00      0.00      0.00         0\n         294       0.00      0.00      0.00         1\n         296       0.00      0.00      0.00         1\n         297       0.00      0.00      0.00         0\n         298       0.00      0.00      0.00         1\n         299       0.00      0.00      0.00         6\n         300       0.00      0.00      0.00         1\n         304       0.00      0.00      0.00         0\n         305       1.00      0.50      0.67         2\n         306       0.00      0.00      0.00         1\n         309       0.00      0.00      0.00         2\n         310       0.00      0.00      0.00         1\n         311       0.00      0.00      0.00         1\n         314       0.00      0.00      0.00         2\n         315       0.00      0.00      0.00         1\n         319       0.00      0.00      0.00         2\n         323       0.00      0.00      0.00         1\n         324       0.00      0.00      0.00         1\n         325       0.00      0.00      0.00         1\n         326       0.00      0.00      0.00         1\n         327       0.00      0.00      0.00         0\n         328       0.00      0.00      0.00         2\n         329       0.00      0.00      0.00         2\n         330       0.00      0.00      0.00         1\n         331       0.00      0.00      0.00         0\n         333       0.00      0.00      0.00         1\n         335       0.00      0.00      0.00         1\n         336       0.00      0.00      0.00         1\n         337       0.00      0.00      0.00         1\n         338       0.00      0.00      0.00         1\n         339       0.00      0.00      0.00         2\n         340       0.00      0.00      0.00         0\n         342       0.00      0.00      0.00         0\n         343       0.00      0.00      0.00         1\n         344       0.00      0.00      0.00         1\n         345       0.00      0.00      0.00         1\n         346       0.00      0.00      0.00         3\n         348       0.00      0.00      0.00         1\n         349       0.00      0.00      0.00         3\n         351       0.00      0.00      0.00         1\n         352       0.00      0.00      0.00         0\n         356       0.00      0.00      0.00         0\n         357       0.00      0.00      0.00         0\n         358       0.00      0.00      0.00         0\n         359       0.00      0.00      0.00         0\n         362       0.00      0.00      0.00         1\n         365       0.00      0.00      0.00         1\n         366       0.00      0.00      0.00         0\n         367       0.00      0.00      0.00         0\n         368       0.00      0.00      0.00         1\n         369       0.00      0.00      0.00         3\n         370       0.00      0.00      0.00         1\n         373       0.00      0.00      0.00         1\n         374       0.00      0.00      0.00         0\n         375       0.00      0.00      0.00         0\n         376       0.00      0.00      0.00         1\n         379       0.00      0.00      0.00         4\n         384       0.00      0.00      0.00         1\n         386       0.00      0.00      0.00         1\n         389       0.00      0.00      0.00         1\n         391       0.00      0.00      0.00         2\n         392       0.00      0.00      0.00         1\n         393       0.00      0.00      0.00         1\n         394       0.00      0.00      0.00         1\n         396       0.00      0.00      0.00         0\n         397       0.00      0.00      0.00         1\n         398       0.00      0.00      0.00         2\n         399       0.00      0.00      0.00         7\n         400       0.00      0.00      0.00         2\n         404       0.00      0.00      0.00         1\n         405       0.00      0.00      0.00         1\n         406       0.00      0.00      0.00         0\n         407       0.00      0.00      0.00         0\n         408       0.00      0.00      0.00         1\n         409       0.00      0.00      0.00         1\n         410       0.00      0.00      0.00         1\n         411       0.00      0.00      0.00         0\n         412       0.00      0.00      0.00         0\n         415       0.00      0.00      0.00         2\n         416       0.00      0.00      0.00         0\n         417       0.00      0.00      0.00         0\n         426       0.00      0.00      0.00         1\n         429       0.00      0.00      0.00         2\n         430       0.00      0.00      0.00         1\n         431       0.00      0.00      0.00         0\n         434       0.00      0.00      0.00         0\n         435       0.00      0.00      0.00         0\n         439       0.00      0.00      0.00         0\n         442       0.00      0.00      0.00         0\n         443       0.00      0.00      0.00         0\n         445       0.00      0.00      0.00         0\n         449       0.00      0.00      0.00         4\n         454       0.00      0.00      0.00         0\n         456       0.00      0.00      0.00         1\n         459       0.00      0.00      0.00         1\n         460       0.00      0.00      0.00         1\n         465       0.00      0.00      0.00         1\n         469       0.00      0.00      0.00         2\n         473       0.00      0.00      0.00         0\n         475       0.00      0.00      0.00         0\n         480       0.00      0.00      0.00         1\n         489       0.00      0.00      0.00         3\n         495       0.00      0.00      0.00         0\n         499       0.00      0.00      0.00         1\n         500       0.00      0.00      0.00         1\n         501       0.00      0.00      0.00         1\n         503       0.00      0.00      0.00         0\n         509       0.00      0.00      0.00         3\n         512       0.00      0.00      0.00         1\n         519       0.00      0.00      0.00         1\n         532       0.00      0.00      0.00         0\n         537       0.00      0.00      0.00         1\n         538       0.00      0.00      0.00         0\n         543       0.00      0.00      0.00         0\n         544       0.00      0.00      0.00         1\n         546       0.00      0.00      0.00         1\n         549       0.00      0.00      0.00         1\n         557       0.00      0.00      0.00         3\n         561       0.00      0.00      0.00         1\n         562       0.00      0.00      0.00         1\n         564       0.00      0.00      0.00         1\n         568       0.00      0.00      0.00         0\n         573       0.00      0.00      0.00         1\n         574       0.00      0.00      0.00         1\n         575       0.00      0.00      0.00         0\n         576       0.00      0.00      0.00         0\n         584       0.00      0.00      0.00         1\n         599       0.00      0.00      0.00         2\n         602       0.00      0.00      0.00         0\n         604       0.00      0.00      0.00         0\n         605       0.00      0.00      0.00         1\n         617       0.00      0.00      0.00         1\n         618       0.00      0.00      0.00         1\n         629       0.00      0.00      0.00         0\n         637       0.00      0.00      0.00         0\n         645       0.00      0.00      0.00         0\n         649       0.00      0.00      0.00         2\n         654       0.00      0.00      0.00         1\n         657       0.00      0.00      0.00         1\n         659       0.00      0.00      0.00         1\n         662       0.00      0.00      0.00         1\n         663       0.00      0.00      0.00         1\n         669       0.00      0.00      0.00         1\n         670       0.00      0.00      0.00         0\n         693       0.00      0.00      0.00         1\n         695       0.00      0.00      0.00         0\n         699       0.00      0.00      0.00         2\n         704       0.00      0.00      0.00         0\n         705       0.00      0.00      0.00         0\n         708       0.00      0.00      0.00         0\n         718       0.00      0.00      0.00         1\n         743       0.00      0.00      0.00         0\n         745       0.00      0.00      0.00         1\n         749       0.00      0.00      0.00         1\n         758       0.00      0.00      0.00         1\n         759       0.00      0.00      0.00         1\n         782       0.00      0.00      0.00         0\n         783       0.00      0.00      0.00         0\n         785       0.00      0.00      0.00         1\n         788       0.00      0.00      0.00         1\n         799       0.00      0.00      0.00         1\n         808       0.00      0.00      0.00         0\n         818       0.00      0.00      0.00         0\n         819       0.00      0.00      0.00         1\n         824       0.00      0.00      0.00         0\n         849       0.00      0.00      0.00         1\n         859       0.00      0.00      0.00         1\n         862       0.00      0.00      0.00         0\n         873       0.00      0.00      0.00         0\n         880       0.00      0.00      0.00         0\n         888       0.00      0.00      0.00         1\n         899       0.00      0.00      0.00         2\n         901       0.00      0.00      0.00         1\n         908       0.00      0.00      0.00         1\n         932       0.00      0.00      0.00         1\n         949       0.00      0.00      0.00         2\n         954       0.00      0.00      0.00         1\n         959       0.00      0.00      0.00         0\n         980       0.00      0.00      0.00         1\n         999       0.00      0.00      0.00         3\n        1002       0.00      0.00      0.00         0\n        1005       0.00      0.00      0.00         1\n        1010       0.00      0.00      0.00         1\n        1024       0.00      0.00      0.00         1\n        1028       0.00      0.00      0.00         0\n        1029       0.00      0.00      0.00         1\n        1059       0.00      0.00      0.00         1\n        1131       0.00      0.00      0.00         0\n        1149       0.00      0.00      0.00         2\n        1166       0.00      0.00      0.00         0\n        1177       0.00      0.00      0.00         0\n        1190       0.00      0.00      0.00         0\n        1199       0.00      0.00      0.00         2\n        1200       0.00      0.00      0.00         0\n        1203       0.00      0.00      0.00         1\n        1205       0.00      0.00      0.00         1\n        1215       0.00      0.00      0.00         1\n        1219       0.00      0.00      0.00         1\n        1300       0.00      0.00      0.00         1\n        1317       0.00      0.00      0.00         0\n        1318       0.00      0.00      0.00         0\n        1319       0.00      0.00      0.00         0\n        1336       0.00      0.00      0.00         1\n        1339       0.00      0.00      0.00         0\n        1399       0.00      0.00      0.00         1\n        1449       0.00      0.00      0.00         1\n        1464       0.00      0.00      0.00         1\n        1499       0.00      0.00      0.00         1\n        1505       0.00      0.00      0.00         0\n        1571       0.00      0.00      0.00         1\n        1595       0.00      0.00      0.00         0\n        1646       0.00      0.00      0.00         0\n        1664       0.00      0.00      0.00         1\n        1698       0.00      0.00      0.00         0\n        1699       0.00      0.00      0.00         0\n        1724       0.00      0.00      0.00         1\n        1740       0.00      0.00      0.00         2\n        1782       0.00      0.00      0.00         0\n        1806       0.00      0.00      0.00         0\n        1828       0.00      0.00      0.00         1\n        1915       0.00      0.00      0.00         0\n        1999       0.00      0.00      0.00         0\n        2098       0.00      0.00      0.00         1\n        2112       0.00      0.00      0.00         0\n        2119       0.00      0.00      0.00         2\n        2120       0.00      0.00      0.00         1\n        2135       0.00      0.00      0.00         1\n        2199       0.00      0.00      0.00         0\n        2255       0.00      0.00      0.00         1\n        2286       0.00      0.00      0.00         1\n        2295       0.00      0.00      0.00         1\n        2449       0.00      0.00      0.00         0\n        2464       0.00      0.00      0.00         0\n        2499       0.00      0.00      0.00         0\n        2520       0.00      0.00      0.00         0\n        2521       0.00      0.00      0.00         0\n        2549       0.00      0.00      0.00         0\n        2562       0.00      0.00      0.00         1\n        2608       0.00      0.00      0.00         0\n        2633       0.00      0.00      0.00         0\n        2695       0.00      0.00      0.00         1\n        2727       0.00      0.00      0.00         2\n        2732       0.00      0.00      0.00         1\n        2750       0.00      0.00      0.00         2\n        2799       0.00      0.00      0.00         2\n        2800       0.00      0.00      0.00         0\n        2802       0.00      0.00      0.00         0\n        2899       0.00      0.00      0.00         1\n        2920       0.00      0.00      0.00         1\n        2949       0.00      0.00      0.00         0\n        2999       0.00      0.00      0.00         1\n        3099       0.00      0.00      0.00         0\n        3335       0.00      0.00      0.00         2\n        3551       0.00      0.00      0.00         0\n        3698       0.00      0.00      0.00         0\n        3699       0.00      0.00      0.00         1\n        3798       0.00      0.00      0.00         0\n        3999       0.00      0.00      0.00         1\n        4075       0.00      0.00      0.00         1\n        4183       0.00      0.00      0.00         1\n        4199       0.00      0.00      0.00         1\n        4311       0.00      0.00      0.00         0\n        4421       0.00      0.00      0.00         0\n        4800       0.00      0.00      0.00         2\n        4967       0.00      0.00      0.00         0\n        5000       0.00      0.00      0.00         2\n        5299       0.00      0.00      0.00         1\n        5600       0.00      0.00      0.00         0\n        5649       0.00      0.00      0.00         2\n        5949       0.00      0.00      0.00         1\n        5990       0.50      1.00      0.67         1\n        6249       0.00      0.00      0.00         0\n        7800       0.00      0.00      0.00         0\n        7900       0.00      0.00      0.00         0\n       13280       0.00      0.00      0.00         0\n\n    accuracy                           0.04      7401\n   macro avg       0.02      0.02      0.02      7401\nweighted avg       0.05      0.04      0.04      7401\n\n",
      "[[1 0 0 ... 0 0 0]\n [0 2 0 ... 0 0 0]\n [0 0 2 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# clf_SVD = Pipeline([('vect', column_transformer),\n",
    "#                     ('svd', TruncatedSVD()),\n",
    "#                    ('clf', RandomForestClassifier())])\n",
    "# # Fit the classifier.\n",
    "# clf_SVD = clf_SVD.fit(X_train, y_train)\n",
    "# # show_metrics(clf_SVD, X_test, y_test)\n",
    "# \n",
    "# ytest_RF = np.array(y_test)\n",
    "# \n",
    "# print(classification_report(ytest_RF, clf_SVD.predict(X_test)))\n",
    "# print(confusion_matrix(ytest_RF, clf_SVD.predict(X_test)))\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}