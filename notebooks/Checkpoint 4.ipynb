{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60462dd1-07f4-46c2-9606-f0f4b4d75dea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint 4 - Multi-Class Classification of Walmart Product Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This checkpoint report summarize's our group's attempts at improving the model performance for the multi-class classification of Walmart product data. In this report, we discuss the performance of our candidate algorithm, the steps taken to tune its performance, and compare the algorithm to a series of other supervised learning models.\n",
    "\n",
    "We explore the following models:\n",
    "\n",
    "1. $k$-Nearest Neighbors\n",
    "1. Logistic Regression\n",
    "1. RBF (Radial Basis-Function) SVC\n",
    "1. Random Forest Classifier (Core Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950b5eb-b762-4c2a-879b-aa50ab942911",
   "metadata": {},
   "source": [
    "As this checkpoint will provide an update over our previous checkpoint's work, the focus of the report has been placed on discussion of the model selection and tuning work. The final presentation will include additional details related to the *formal problem definition*, *key issues*, *related work*, *validation*, *key contributions*, and *future work*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbab89d-0767-4c3f-8ebf-81a6d2b228e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\\Library\\My Repositories\\rit\\2211_FALL\\ISTE780\\Project\n"
     ]
    }
   ],
   "source": [
    "# Enable hot-reloading of external scripts.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set project directory to project root.\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().resolve().parents[0]\n",
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbd2325-1ff2-479e-b76c-77e5f18968da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "# Import utilities.\n",
    "from src.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9838f-b861-43dc-93ae-cf1756ee6f70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "> Can we accurately predict product prices using textual features from the Walmart product dataset?\n",
    "\n",
    "Our group was interested in predicting price ranges based on textual features (eg., `name`, `brand`, `description`, etc.) sourced from a dataset of ~30,000 Walmart product details, scraped by [`PromptCloud.com`](https://www.promptcloud.com/) and [hosted on Kaggle](https://www.kaggle.com/promptcloud/walmart-product-dataset-usa).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Although regression could be used to predict accurate prices, we were interested in the challenges posed by multi-class categorization of text data.\n",
    "\n",
    "Ideally, a small number of explainable price ranges could be used to clearly communicate product price outcomes to an ideal user of our system: a small-business owner interested in identifying products that can be realistically and competitively priced against larger big-brand department stores (eg. Walmart, Amazon, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2dee0-df7e-44c8-a752-c6383be1e52e",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "The original dataset consists of ~30,000 entries representing a sample of products Walmart had listed online in 2019, at the time of `PromptCloud`'s data scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9d3b04-c297-4b55-84bf-27e2a23c735b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('A:/Library/My Repositories/rit/2211_FALL/ISTE780/Project/data/interim/ecommerce_data-cleaned-0.1.4.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29604 entries, 0 to 29999\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   brand         29604 non-null  object \n",
      " 1   name          29604 non-null  object \n",
      " 2   description   29604 non-null  object \n",
      " 3   category_1    29604 non-null  object \n",
      " 4   category_2    29604 non-null  object \n",
      " 5   category_3    29604 non-null  object \n",
      " 6   keywords      29604 non-null  object \n",
      " 7   price_raw     29604 non-null  float64\n",
      " 8   discount_raw  29604 non-null  float64\n",
      " 9   price_range   29604 non-null  object \n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 2.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "products_uri = get_interim_filepath(\"0.1.4\", tag=\"cleaned\")\n",
    "products = pd.read_csv(products_uri, index_col=0, keep_default_na=False)\n",
    "display(products_uri)\n",
    "display(products.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb8206-051b-408d-b34d-0174ce74df15",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26f418-c02b-419b-8298-11e0c66e9cd8",
   "metadata": {},
   "source": [
    "We performed the following preprocessing steps to prepare the data:\n",
    "\n",
    "1. Removed `Walmart`-specific and redundant fields.\n",
    "1. Reorganized and renamed field names for clarity.\n",
    "1. Extracted and engineered new features from `category_raw`:\n",
    "    1. `category_1`, containing the primary category.\n",
    "    1. `category_2`, containing the secondary category.\n",
    "    1. `category_3`, containing the tertiary category.\n",
    "    1. `keywords`, containing category keywords that could not be placed in the previous category features.\n",
    "1. Cleaned textual features in the dataset:\n",
    "    1. Removed unrecognized characters, punctuation.\n",
    "    1. Removed stopwords (eg. \"a\", \"the\", etc.) using the `ntlk` English stop words.\n",
    "    1. Tokenized and stemmed language using a `PorterStemmer` from the `nltk` package.\n",
    "    1. Normalized text to lowercase.\n",
    "1. Extracted and engineered price ranges from the `price_raw` feature. Class labels stored in `price_range`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bf510-ebc9-4e88-ab70-8abe91b9bed9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We performed exploratory data analysis on the terms in the dataset. This was reported on in the previous checkpoint and will be shown again in the final presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6aac5b-0e9c-4f01-b676-b7326a9e08ec",
   "metadata": {},
   "source": [
    "### Price Range Response\n",
    "\n",
    "In our previous checkpoint, we attempted a multi-class classification problem on `10` non-overlapping price regions. This resulted in serious deficiencies in model performance. In preparation for this checkpoint, we performed a histogram analysis with a smaller number of 'bins', settling on `4` non-overlapping price regions:\n",
    "\n",
    "1. `(25, 50]`\n",
    "1. `(0, 25]`\n",
    "1. `(100, 100+]`\n",
    "1. `(50, 100]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeec136-f488-4659-9437-bfef3dee5c63",
   "metadata": {},
   "source": [
    "### Feature Dictionary\n",
    "\n",
    "This is a challenging problem because all of the predictive features in the dataset are purely organized text data. The predictive text features we are working with are:\n",
    "\n",
    "1. `brand` - The product's listed brand.\n",
    "1. `name` - The product's listed name.\n",
    "1. `description` - Walmart's full, human-readable text description of the product.\n",
    "1. `category_1` - Extracted feature representing the primary category reported in `category_raw`.\n",
    "1. `category_2` - Extracted feature representing the secondary category reported in `category_raw`.\n",
    "1. `category_3` - Extracted feature representing the tertiary category reported in `category_raw`.\n",
    "1. `keywords` - Extracted feature representing additional text details provided in `category_raw`.\n",
    "\n",
    "The `category_raw` feature is a column included in the source dataset that contained a formatted string representing the product's category details, delimited by a `|` (pipe) character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce038b0b-ccdc-4a82-be51-5c7905142b5a",
   "metadata": {},
   "source": [
    "## Pipeline Setup\n",
    "\n",
    "The `sklearn` library provides extensive support for data science pipelines through the use of the `Pipeline`, `FeatureUnion`, and `ColumnTransformer` pipeline composition tools. We use these utilities (among others) in order to create our classifier models, measure performance, and report scores. This section describes our preparation work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3efb3-303b-4e27-be56-e853317c0efc",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We use a majority of the features present in the preprocessed dataset we import. The following step will exclude the response from the dataset and drop two unused columns: `price_raw` and `discount_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb43ff2-40f7-438b-875c-50ae9fc4d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29604 entries, 0 to 29999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   brand        29604 non-null  object\n",
      " 1   name         29604 non-null  object\n",
      " 2   description  29604 non-null  object\n",
      " 3   category_1   29604 non-null  object\n",
      " 4   category_2   29604 non-null  object\n",
      " 5   category_3   29604 non-null  object\n",
      " 6   keywords     29604 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Create list with features to use.\n",
    "features = [ \n",
    "    'brand', 'name', 'description', \n",
    "    'category_1', 'category_2', 'category_3',\n",
    "    'keywords']\n",
    "\n",
    "# Select feature columns only.\n",
    "X = products.loc[:, features]\n",
    "\n",
    "# Display the information about the features dataframe.\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a91e7-343e-46f1-ade8-ed8738722ca6",
   "metadata": {},
   "source": [
    "### Response Encoding\n",
    "\n",
    "In order to utilize `sklearn`'s classifiers, we encoded the categorical response variable `price_range` using the `LabelEncoder` preprocessing utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5551f70c-d2be-4e4b-a252-3b0682b00e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29604 entries, 0 to 29603\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   y       29604 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 115.8 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(25, 50]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 25]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(100, 100+]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(50, 100]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label  Class\n",
       "0     (25, 50]      0\n",
       "1      (0, 25]      1\n",
       "2  (100, 100+]      2\n",
       "3    (50, 100]      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels \n",
    "label_encoder = LabelEncoder()\n",
    "labels = products.loc[:,\"price_range\"]\n",
    "y = label_encoder.fit_transform(labels)\n",
    "display(pd.DataFrame({'y': y}).info())\n",
    "\n",
    "# Display the unique labels and codes.\n",
    "labels = pd.DataFrame({'Label': labels.unique(), 'Class': np.unique(y)})\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91989688-de86-429b-9e5e-2666323be92f",
   "metadata": {},
   "source": [
    "### Subset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03279dc2-f9e0-486c-8369-e809f59a4dc7",
   "metadata": {},
   "source": [
    "In order to estimate how our models will perform on new, previously unseen data, we fit our models on a training subset and test them on a held-out validation subset. Due to the imbalanced distribution of price ranges by category class, we ensure that our splits are stratified. The `train_test_split` function provided by the `sklearn.model_selection` package allows us to split our data while respecting the distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553a3ebf-cd26-404c-b12f-21eb018bebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare split percentages.\n",
    "pct_train = 0.20\n",
    "pct_test = 1 - pct_train\n",
    "\n",
    "# Create a train-test split, samples stratified by class.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = pct_test, random_state = 20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62eec4f-6b33-4ed3-a499-9d72ee136d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X train: (5920, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5 largest category value counts in training set: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sport outdoor    2188\n",
       "food              809\n",
       "health            755\n",
       "babi              562\n",
       "person care       467\n",
       "Name: category_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y train: (5920,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Breakdown of training set classes: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class (train)\n",
       "0                3333\n",
       "2                1248\n",
       "3                 683\n",
       "1                 656\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display details about each train split.\n",
    "display(f\"X train: {X_train.shape}\")\n",
    "display(\"5 largest category value counts in training set: \")\n",
    "display(X_train['category_1'].value_counts().nlargest(5))\n",
    "display(f\"y train: {y_train.shape}\")\n",
    "display(\"Breakdown of training set classes: \")\n",
    "display(pd.DataFrame({'Class (train)': y_train}).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822d4cba-7011-4213-8262-9c1cdd8f7845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X test: (23684, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5 largest category value counts in testing set: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sport outdoor    8775\n",
       "health           3152\n",
       "food             3128\n",
       "babi             2147\n",
       "person care      1836\n",
       "Name: category_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y test: (23684,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Breakdown of testing set classes: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class (test)\n",
       "0               13337\n",
       "2                4991\n",
       "3                2733\n",
       "1                2623\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display details about each test split.\n",
    "display(f\"X test: {X_test.shape}\")\n",
    "display(\"5 largest category value counts in testing set: \")\n",
    "display(X_test['category_1'].value_counts().nlargest(5))\n",
    "display(f\"y test: {y_test.shape}\")\n",
    "display(\"Breakdown of testing set classes: \")\n",
    "display(pd.DataFrame({'Class (test)': y_test}).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181cbb4-67b6-48bf-b4ec-6f0cf14a279d",
   "metadata": {},
   "source": [
    "### Pipeline Composition\n",
    "\n",
    "The `Pipeline` concept in `sklearn` is an intuitive way to build reusable transformation-classifier workflows, reducing the chance for human error during one of the 'in-between' steps. We simply create our `Pipeline` object by specifying the steps using a python `list` of `tuple`s and then `fit` the resulting pipeline object with our input data.\n",
    "\n",
    "The typical workflow for our project consists of:\n",
    "\n",
    "1. Applying a `TfidfVectorizer` transformation to all text features in the dataset.\n",
    "1. Performing some set of dimensionality reduction on the transformed TF-IDF features (eg., `SelectKBest`).\n",
    "1. Fitting a classifier of interest.\n",
    "\n",
    "The classifier of interest and dimensionality reduction techniques chosen are places where we can attempt to optimize our prediction capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7be28b-5259-45d7-863b-54c601e5ff65",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "\n",
    "The `TfidfVectorizer` converts a collection of raw documents to a matrix of TF-IDF features. The transformer's resulting [TF-IDF](https://www.kdnuggets.com/2018/08/wtf-tf-idf.html) features will help surface the stems that identify each product, which can be useful if the most frequent terms are not necessarily strong predictors due to re-use in each product.\n",
    "\n",
    "For example, each description Walmart uses includes the same boilerplate introductory text. When the training samples are fit, the transformer will recognize this and other, rarer features will be more prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71e47c0-5eee-45a5-8337-425c390fa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the vectorizer that will be fit on the features.\n",
    "def get_vectorizer(feature, **params):\n",
    "    \"\"\"Compose Tuple for feature and its specific vectorizer.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    return (feature, vectorizer, feature)\n",
    "\n",
    "# Create the transformers for each text feature.\n",
    "def get_transformers(columns, feature_params):\n",
    "    \"\"\"\n",
    "    Create transformers for the ColumnTransformer.\n",
    "    @param columns List of features.\n",
    "    @param feature_params Dictionary of vectorizer params for each feature.\n",
    "    \"\"\"\n",
    "    return [get_vectorizer(feature, **feature_params[feature]) for feature in columns]\n",
    "    \n",
    "# Create the transformer parameter dictionary.\n",
    "def get_vectorizer_params(**kwargs):\n",
    "    default_params = {\n",
    "        'sublinear_tf': True,\n",
    "        'stop_words': 'english',\n",
    "        'strip_accents': 'ascii',\n",
    "        'norm': 'l2',\n",
    "        'ngram_range': (1,1),\n",
    "        'max_features': 10000,\n",
    "    }\n",
    "    return {**default_params, **kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224afefc-9813-469b-a50d-6df48f131bd5",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "Knowing that we can creat a `TfidfVectorizer` for each individual text feature, we can then create a `ColumnTransformer` that combines these transformers into a single transformer that will accept the entire feature matrix and pas the appropriate ones into each appropriate vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2857e3aa-b0c4-4be2-b40e-9c48ea7e43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create the ColumnTransformer.\n",
    "def get_column_transformer():\n",
    "    params = {\n",
    "        'brand': get_vectorizer_params(\n",
    "            ngram_range = (2,2),\n",
    "            max_features = 20000\n",
    "        ),\n",
    "        'name': get_vectorizer_params(\n",
    "            ngram_range = (1,2),\n",
    "            max_features = 20000\n",
    "        ),\n",
    "        'description': get_vectorizer_params(\n",
    "            ngram_range = (2,2),\n",
    "            max_features = 20000\n",
    "        ),\n",
    "        'category_1': get_vectorizer_params(\n",
    "            max_df = 0.99        \n",
    "        ),\n",
    "        'category_2': get_vectorizer_params(\n",
    "            max_df = 0.99    \n",
    "        ),\n",
    "        'category_3': get_vectorizer_params(\n",
    "            max_df = 0.99\n",
    "        ),\n",
    "        'keywords': get_vectorizer_params(\n",
    "            max_features = 20000\n",
    "        ),\n",
    "    }\n",
    "    transformers = get_transformers(list(params.keys()), params)\n",
    "    return ColumnTransformer(transformers, remainder = 'drop', verbose_feature_names_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9188a21-d1d8-41c8-9e44-19fb0cb59655",
   "metadata": {},
   "source": [
    "### SelectKBest\n",
    "\n",
    "For the baseline algorithms, we'll use `SelectKBest` to choose the best categorical features, when tested using `chi2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b28481-ae60-41e8-81b5-663fddcb26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf98d8-3475-4eda-ad12-3f5dac6ea83b",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "The `Pipeline` utility combines the `ColumnTransformer` with a dimensionality reduction tool. We opted to use the `SelectKBest()` tool for our baseline models, but we explore other options with our core algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39123243-a08c-486b-8029-9ca54fce0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_pipeline(estimator, reducer):\n",
    "    \"\"\"Get Pipeline that uses provided estimator and dimensionality reducer.\"\"\"\n",
    "    column_transformer = get_column_transformer()\n",
    "    return Pipeline([\n",
    "        (\"vecs\", column_transformer),\n",
    "        (\"dims\", reducer),\n",
    "        (\"clf\", estimator)\n",
    "    ])\n",
    "\n",
    "def get_default_pipeline(estimator):\n",
    "    \"\"\"Get Pipeline that uses provided estimator.\"\"\"\n",
    "    kbest = SelectKBest(chi2, k = 7000)\n",
    "    return get_pipeline(estimator, kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48bcb1b6-8b3b-4166-8380-b5d5bfeb639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vecs',\n",
       "                 ColumnTransformer(transformers=[('brand',\n",
       "                                                  TfidfVectorizer(max_features=20000,\n",
       "                                                                  ngram_range=(2,\n",
       "                                                                               2),\n",
       "                                                                  stop_words='english',\n",
       "                                                                  strip_accents='ascii',\n",
       "                                                                  sublinear_tf=True),\n",
       "                                                  'brand'),\n",
       "                                                 ('name',\n",
       "                                                  TfidfVectorizer(max_features=20000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  stop_words='english',\n",
       "                                                                  strip_accents='ascii',\n",
       "                                                                  sublinear_tf=True),\n",
       "                                                  'name'),\n",
       "                                                 ('description',\n",
       "                                                  TfidfVectorizer...\n",
       "                                                 ('category_3',\n",
       "                                                  TfidfVectorizer(max_df=0.99,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  stop_words='english',\n",
       "                                                                  strip_accents='ascii',\n",
       "                                                                  sublinear_tf=True),\n",
       "                                                  'category_3'),\n",
       "                                                 ('keywords',\n",
       "                                                  TfidfVectorizer(max_features=20000,\n",
       "                                                                  stop_words='english',\n",
       "                                                                  strip_accents='ascii',\n",
       "                                                                  sublinear_tf=True),\n",
       "                                                  'keywords')])),\n",
       "                ('dims',\n",
       "                 SelectKBest(k=7000,\n",
       "                             score_func=<function chi2 at 0x00000254CF33A8B0>)),\n",
       "                ('clf', 'passthrough')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display example steps of the pipeline, with a \"passthrough\" estimator.\n",
    "get_default_pipeline(\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119f38e-8dd6-4175-b741-0ed3581bf788",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "In order to compare our models, we need a reasonable baseline and a benchmarking function that will tell us how long it takes each model to fit on a training sample and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9a4b929-d23c-474a-9310-1719ab7c4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Report sizes.\n",
    "def show_size(docs, label):\n",
    "    print(f\"> {docs.shape[0]} samples in {label}.\")\n",
    "    \n",
    "# Benchmark pipeline.\n",
    "def benchmark(label, pipeline, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Benchmark a pipeline with appropriate train and test data.\"\"\"\n",
    "        \n",
    "    # Fitting the pipeline.\n",
    "    print(\"_\" * 80)\n",
    "    print(f\"Executing pipeline {label}...\")\n",
    "    print(\"Training: \")\n",
    "    if hasattr(pipeline, 'named_steps'):\n",
    "        print(pipeline.named_steps['clf'])\n",
    "    else:\n",
    "        print(pipeline)\n",
    "    show_size(X_train, \"training set\")\n",
    "    train_start = time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_elapsed = time() - train_start\n",
    "    print(\"Finished training pipeline in %0.3f second(s).\" % train_elapsed)\n",
    "    \n",
    "    # Validation of the pipeline performance.\n",
    "    print(\"Making predictions with test set: \")\n",
    "    show_size(X_test, \"test set\")\n",
    "    test_start = time()\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    test_elapsed = time() - test_start\n",
    "    print(\"Finished making predictions in %0.3f second(s).\" % test_elapsed)\n",
    "    \n",
    "    # Print metrics.\n",
    "    test_truth = np.array(y_test)\n",
    "    test_labels = labels['Label'].to_numpy()\n",
    "    score = accuracy_score(test_truth, test_predictions)\n",
    "    error = 1 - score\n",
    "    print(\"Accuracy Score: %0.3f%%\" % (score * 100))\n",
    "    print(\"Misclassification Score: %0.3f%%\" % (error * 100))\n",
    "    \n",
    "    # Print report.\n",
    "    print(\"Classification report: \")\n",
    "    print(classification_report(test_truth, test_predictions, target_names=test_labels, zero_division=0))\n",
    "    \n",
    "    # Print confusion matrix.\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(test_truth, test_predictions))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7519ea2-a968-4d50-b2f0-16186e265d67",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "\n",
    "In order to compare our models to a reasonable baseline, we fit the model features using a `DummyClassifier` that makes predictions using simple rules. This `DummyClassifier` will simply guess the labels based on the class priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae27bf67-f5f5-4cbc-b4e9-049809fe4817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Dummy Classifier...\n",
      "Training: \n",
      "DummyClassifier(strategy='stratified')\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 2.112 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 5.048 second(s).\n",
      "Accuracy Score: 38.494%\n",
      "Misclassification Score: 61.506%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.56      0.56      0.56     13337\n",
      "     (0, 25]       0.10      0.10      0.10      2623\n",
      " (100, 100+]       0.21      0.22      0.21      4991\n",
      "   (50, 100]       0.11      0.11      0.11      2733\n",
      "\n",
      "    accuracy                           0.38     23684\n",
      "   macro avg       0.25      0.25      0.25     23684\n",
      "weighted avg       0.38      0.38      0.38     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[7469 1493 2871 1504]\n",
      " [1499  269  564  291]\n",
      " [2812  531 1075  573]\n",
      " [1528  302  599  304]]\n"
     ]
    }
   ],
   "source": [
    "# Import the dummy classifier.\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Benchmark the dummy classifier.\n",
    "clf_dummy = get_default_pipeline(DummyClassifier(strategy='stratified'))\n",
    "benchmark(\"Dummy Classifier\", clf_dummy, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605239f6-2485-4e0e-83ba-6ad724872a54",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The dummy classifier serves as a useful baseline: it is something to compare our models' performance against. We can see that by making guesses for the price range based on the class distribution, we see an accuracy of $\\approx 56$%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ee804-fbd6-4be1-bebb-8e67e970dad0",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbor Classifier\n",
    "\n",
    "K-Nearest Neighbor (KNN) is a non-parametric classification algorithm that tries to classify a given observation to a response class with the highest estimated probability. For a given positive value of K, the classifier identifies K points from the training data set that are closest to the test observation (i.e. it’s K nearest neighbors). Then it computes the estimated conditional probability using the Bayes rule and classifies the test observation to the response class with the largest probability. In our project, KNN can be used to model the List Price of a Walmart product by finding the K-nearest neighbors and assigning the list price label that has the highest estimated probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "635af976-18eb-454a-9a90-fe81d2ee52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline k-Nearest Neighbors Classifier...\n",
      "Training: \n",
      "KNeighborsClassifier()\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 2.007 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 8.512 second(s).\n",
      "Accuracy Score: 59.884%\n",
      "Misclassification Score: 40.116%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.61      0.96      0.75     13337\n",
      "     (0, 25]       0.68      0.22      0.33      2623\n",
      " (100, 100+]       0.38      0.13      0.19      4991\n",
      "   (50, 100]       0.54      0.08      0.14      2733\n",
      "\n",
      "    accuracy                           0.60     23684\n",
      "   macro avg       0.55      0.35      0.35     23684\n",
      "weighted avg       0.56      0.60      0.51     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12756    51   505    25]\n",
      " [ 1721   568   230   104]\n",
      " [ 4198    93   644    56]\n",
      " [ 2079   124   315   215]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Benchmark the KNN classifier with default settings.\n",
    "clf_kNN = get_default_pipeline(KNeighborsClassifier())\n",
    "benchmark(\"k-Nearest Neighbors Classifier\", clf_kNN, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3251131-84ab-4109-af44-d3eb55fb5596",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical model that can be used to model the probability that the response Y belongs to a particular category/class. This is different from other classification algorithms that model the response Y directly. In our project, Logistic Regression can be used to model the probability that the List Price of a Walmart product belongs to any of the labels. Logistic Regression uses a logistic function to model a statistically dependent variable (typically binary). In a binary logistic regression problem, the dependent variable (i.e., the response Y) can have two possible categorical values such as “0” and “1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f71af7f8-5e1b-4e15-a877-84bacc235d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Logistic Regression Classifier...\n",
      "Training: \n",
      "LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 3.019 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 4.695 second(s).\n",
      "Accuracy Score: 63.511%\n",
      "Misclassification Score: 36.489%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.68      0.93      0.78     13337\n",
      "     (0, 25]       0.65      0.45      0.53      2623\n",
      " (100, 100+]       0.41      0.21      0.28      4991\n",
      "   (50, 100]       0.43      0.16      0.24      2733\n",
      "\n",
      "    accuracy                           0.64     23684\n",
      "   macro avg       0.54      0.44      0.46     23684\n",
      "weighted avg       0.59      0.64      0.59     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12355   111   725   146]\n",
      " [  939  1169   296   219]\n",
      " [ 3476   216  1071   228]\n",
      " [ 1477   306   503   447]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Benchmark the classifier.\n",
    "clf_logreg = get_default_pipeline(LogisticRegression(multi_class='multinomial', max_iter=1000))\n",
    "benchmark(\"Logistic Regression Classifier\", clf_logreg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71465e-38fc-4f47-a6fc-a4aaa723be7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RBF (Radial Basis Function) SVC\n",
    "\n",
    "SVC stands for C-Support Vector Classification. According to skcikit learn, \"The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples.\" SVC is using a radial basis function for its kernel to build a \"one vs one\" model. \n",
    "\n",
    "Support Vector Machines (SVMs) are used for solving supervised learning classification problems, but they can also be used for clustering and regression algorithms. SVM tries to find a hyperplane that separates the response classes with highest margin possible. The points that lie on the margins are called support vectors. SVM uses a kernel called radial basis function to build a one vs one model for the prediction with approximately 43% accuracy. RBF is the default kernel used within scikit-learn’s SVM algorithm, and it helps to control individual observation’s effect on the overall algorithm. Large values of gamma parameter indicate greater effect of test observation on the overall algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9277dc8d-6965-4fc1-b864-cca78e11b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Radial Basis Function SVC Classifier...\n",
      "Training: \n",
      "SVC(C=1, decision_function_shape='ovo', gamma=1)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 6.598 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 15.408 second(s).\n",
      "Accuracy Score: 62.080%\n",
      "Misclassification Score: 37.920%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.67      0.92      0.77     13337\n",
      "     (0, 25]       0.70      0.36      0.47      2623\n",
      " (100, 100+]       0.37      0.23      0.28      4991\n",
      "   (50, 100]       0.47      0.14      0.21      2733\n",
      "\n",
      "    accuracy                           0.62     23684\n",
      "   macro avg       0.55      0.41      0.44     23684\n",
      "weighted avg       0.58      0.62      0.57     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12239    91   885   122]\n",
      " [ 1012   942   499   170]\n",
      " [ 3585   127  1143   136]\n",
      " [ 1560   193   601   379]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the pipeline.\n",
    "clf_RBF_SVC = get_default_pipeline(SVC(kernel = 'rbf', gamma=1, C=1, decision_function_shape='ovo'))\n",
    "benchmark(\"Radial Basis Function SVC Classifier\", clf_RBF_SVC, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ef884-3b3a-4d72-80bb-1e235258e9bc",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (Core Algorithm)\n",
    "\n",
    "The random forest classifier is an ensemble estimator that fits a series of decision trees on various sub-samples of the dataset. `sklearn`'s implementation uses bootstrapping by default and uses the `gini` index as a measure of node purity in each of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db73ff-8407-4bde-8324-bdceed690d06",
   "metadata": {},
   "source": [
    "### Baseline Performance\n",
    "\n",
    "For reference, we fit a single `DecisionTreeClassifier` prior to the `RandomForestClassifier` to compare the gains from bagging and randomizing the features used for each split. `DecisionTree` is a non-paramedic supervised learning model used for classification as well as regression problems. The interpretability of this model is the main reason for its use. Here, the motive of using `DecisionTree` is to understand the important features and how they influence the accuracy of the model. We observed that features `keywords`, `category_3` and `brand` have a high importance. We also use decision tree model to predict the price ranges of items from Walmart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41f0ec64-903e-4f32-a607-9bd88a7689b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Decision Tree Classifier...\n",
      "Training: \n",
      "DecisionTreeClassifier(random_state=133)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 2.607 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 4.481 second(s).\n",
      "Accuracy Score: 57.068%\n",
      "Misclassification Score: 42.932%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.69      0.78      0.73     13337\n",
      "     (0, 25]       0.46      0.40      0.43      2623\n",
      " (100, 100+]       0.33      0.30      0.31      4991\n",
      "   (50, 100]       0.30      0.20      0.24      2733\n",
      "\n",
      "    accuracy                           0.57     23684\n",
      "   macro avg       0.45      0.42      0.43     23684\n",
      "weighted avg       0.55      0.57      0.56     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[10412   501  1904   520]\n",
      " [  746  1061   459   357]\n",
      " [ 2705   380  1490   416]\n",
      " [ 1162   360   658   553]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Benchmark the classifier.\n",
    "clf_tree = get_default_pipeline(DecisionTreeClassifier(random_state=133))\n",
    "benchmark(\"Decision Tree Classifier\", clf_tree, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24a200-f0af-4e7d-9449-afa5e7e876a0",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (Default Settings)\n",
    "\n",
    "The random forest algorithm is fit and reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53a514ba-cd4c-414c-9b98-0ff531629faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Default Random Forest Classifier...\n",
      "Training: \n",
      "RandomForestClassifier(random_state=320)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 6.686 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 5.575 second(s).\n",
      "Accuracy Score: 62.861%\n",
      "Misclassification Score: 37.139%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.66      0.93      0.77     13337\n",
      "     (0, 25]       0.74      0.35      0.48      2623\n",
      " (100, 100+]       0.41      0.23      0.30      4991\n",
      "   (50, 100]       0.51      0.15      0.23      2733\n",
      "\n",
      "    accuracy                           0.63     23684\n",
      "   macro avg       0.58      0.42      0.44     23684\n",
      "weighted avg       0.60      0.63      0.58     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12410    41   796    90]\n",
      " [ 1212   920   334   157]\n",
      " [ 3596   106  1143   146]\n",
      " [ 1656   180   482   415]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Benchmark the classifier.\n",
    "clf_tree_RF = get_default_pipeline(RandomForestClassifier(random_state=320))\n",
    "benchmark(\"Default Random Forest Classifier\", clf_tree_RF, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd97e5-5577-47e5-bccb-5bf946942660",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (SelectKBest(k='all'))\n",
    "\n",
    "In an attempt to improve performance, we tried removing the `SelectKBest()` limitation, setting the `k` parameter to `all` in the dimension reduction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bda38ae7-8043-4262-bde2-63a9bb66cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Default Random Forest Classifier...\n",
      "Training: \n",
      "RandomForestClassifier(random_state=320)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 8.904 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 6.018 second(s).\n",
      "Accuracy Score: 62.734%\n",
      "Misclassification Score: 37.266%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.64      0.97      0.77     13337\n",
      "     (0, 25]       0.70      0.33      0.45      2623\n",
      " (100, 100+]       0.49      0.14      0.21      4991\n",
      "   (50, 100]       0.50      0.13      0.21      2733\n",
      "\n",
      "    accuracy                           0.63     23684\n",
      "   macro avg       0.58      0.39      0.41     23684\n",
      "weighted avg       0.60      0.63      0.55     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12937    59   275    66]\n",
      " [ 1433   868   166   156]\n",
      " [ 4048   118   685   140]\n",
      " [ 1913   190   262   368]]\n"
     ]
    }
   ],
   "source": [
    "# Benchmark the classifier.\n",
    "select_all = SelectKBest(chi2, k='all')\n",
    "clf_tree_RF = get_pipeline(RandomForestClassifier(random_state=320), select_all)\n",
    "benchmark(\"Default Random Forest Classifier\", clf_tree_RF, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66feb012-706d-4403-9486-81cc516ee48a",
   "metadata": {},
   "source": [
    "### AdaBoost Decision Tree Classifier (SAMME)\n",
    "\n",
    "In order to further optimize and  improve the prediction accuracy of the decision tree model, we used the multi-class Adaboost classifier. In this method, we use the Adaboost classifier on top of the decision tree classifier to fit a sequence of weak learners on repeatedly modified versions of the data. The final prediction is based on a weighted majority vote from a combination of all of the smaller decision trees. By using the Stagewise Additive Modeling (SAMME) discrete-valued algorithm (meaning that it outputs either 0 or 1)  we get a better accuracy of 62% than the original decision tree model. We also tested the SMME.R algorithm which is a variation of the SAMME algorithm that outputs a “real” valued number using class probabilities. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. However, the SAMME algorithm performed better than the SAMME.R algorithm for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3ff6916-da19-407d-b5ef-7588f2857e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline AdaBoost (SAMME)...\n",
      "Training: \n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "                   base_estimator=DecisionTreeClassifier(max_depth=2),\n",
      "                   learning_rate=1.5, n_estimators=600)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 39.754 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 9.985 second(s).\n",
      "Accuracy Score: 59.331%\n",
      "Misclassification Score: 40.669%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.60      0.98      0.74     13337\n",
      "     (0, 25]       0.66      0.20      0.31      2623\n",
      " (100, 100+]       0.41      0.05      0.09      4991\n",
      "   (50, 100]       0.54      0.05      0.10      2733\n",
      "\n",
      "    accuracy                           0.59     23684\n",
      "   macro avg       0.55      0.32      0.31     23684\n",
      "weighted avg       0.56      0.59      0.48     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[13131    53   127    26]\n",
      " [ 1933   532   115    43]\n",
      " [ 4600    92   246    53]\n",
      " [ 2355   127   108   143]]\n"
     ]
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Benchmark the classifier.\n",
    "clf_tree_ADA = get_pipeline(\n",
    "    AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(max_depth=2),\n",
    "        n_estimators = 600,\n",
    "        learning_rate = 1.5,\n",
    "        algorithm = \"SAMME\"\n",
    "    ), select_all)\n",
    "benchmark(\"AdaBoost (SAMME)\", clf_tree_ADA, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c7e0-4242-4a25-9204-5b578f8907b5",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (Truncated SVD)\n",
    "\n",
    "Dimensionality reduction the process of reducing the dimension of our feature set. Our feature set could be a data set with hundred columns (i.e features) or it could be an array of points that make up a large sphere in the three-dimensional space. Dimensionality reduction is the process of bringing the number of columns down considerably, like twenty columns or converting the sphere to a circle in the two-dimensional space. For our project, we tried different dimensionality reduction algorithms such as `TruncatedSVD` and `LatentDirichletAllocation` (LDA).\n",
    "\n",
    "TruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the k largest singular values, where k is a user-specified parameter. When truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer), this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a “semantic” space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d497bd1-eae0-403a-be33-39c484de34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Random Forest Classifier (Truncated SVD)...\n",
      "Training: \n",
      "RandomForestClassifier(random_state=320)\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 39.791 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 6.209 second(s).\n",
      "Accuracy Score: 62.912%\n",
      "Misclassification Score: 37.088%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.64      0.96      0.77     13337\n",
      "     (0, 25]       0.69      0.38      0.49      2623\n",
      " (100, 100+]       0.47      0.14      0.22      4991\n",
      "   (50, 100]       0.54      0.13      0.21      2733\n",
      "\n",
      "    accuracy                           0.63     23684\n",
      "   macro avg       0.58      0.40      0.42     23684\n",
      "weighted avg       0.60      0.63      0.56     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12845    81   360    51]\n",
      " [ 1339   993   163   128]\n",
      " [ 4024   134   715   118]\n",
      " [ 1872   231   283   347]]\n"
     ]
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Benchmark the classifier.\n",
    "svd = TruncatedSVD(n_components = 1000)\n",
    "clf_RF_SVD = get_pipeline(RandomForestClassifier(random_state=320), svd)\n",
    "benchmark(\"Random Forest Classifier (Truncated SVD)\", clf_RF_SVD, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c81aa-1bdb-4f6e-b3e0-59a0193866a8",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (Latent Dirichlet Allocation)\n",
    "\n",
    "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a27096a9-a6b5-4501-a539-6a52c364f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Random Forest Classifier (Latent Dirichlet Allocation)...\n",
      "Training: \n",
      "RandomForestClassifier(random_state=320)\n",
      "> 5920 samples in training set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Library\\My Repositories\\rit\\2211_FALL\\ISTE780\\Project\\.condaenv\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:861: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-1.0 * perword_bound)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training pipeline in 70.856 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 29.847 second(s).\n",
      "Accuracy Score: 52.208%\n",
      "Misclassification Score: 47.792%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.59      0.84      0.69     13337\n",
      "     (0, 25]       0.27      0.12      0.17      2623\n",
      " (100, 100+]       0.26      0.14      0.18      4991\n",
      "   (50, 100]       0.19      0.07      0.11      2733\n",
      "\n",
      "    accuracy                           0.52     23684\n",
      "   macro avg       0.33      0.29      0.29     23684\n",
      "weighted avg       0.44      0.52      0.46     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[11170   414  1284   469]\n",
      " [ 1781   317   364   161]\n",
      " [ 3874   229   677   211]\n",
      " [ 2001   206   325   201]]\n"
     ]
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Benchmark the classifier.\n",
    "lda = LatentDirichletAllocation(max_iter=1, n_components = 1000)\n",
    "clf_RF_LDA = get_pipeline(RandomForestClassifier(random_state=320), lda)\n",
    "benchmark(\"Random Forest Classifier (Latent Dirichlet Allocation)\", clf_RF_LDA, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef996d-176b-4235-92ef-1b6099c058e9",
   "metadata": {},
   "source": [
    "### Using GridSearch for Optimal Random Forest Classifier\n",
    "\n",
    "In our project, we implemented sklearn.decomposition.TruncatedSVD on our data set using a GridSearch, that runs TruncatedSVD() along with LatentDirichletAllocation() and selectKBest()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cafff54f-8d25-4a75-94da-c8890b2b3012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Executing pipeline Random Forest Classifier (Grid Search)...\n",
      "Training: \n",
      "GridSearchCV(estimator=Pipeline(steps=[('vecs',\n",
      "                                        ColumnTransformer(transformers=[('brand',\n",
      "                                                                         TfidfVectorizer(max_features=20000,\n",
      "                                                                                         ngram_range=(2,\n",
      "                                                                                                      2),\n",
      "                                                                                         stop_words='english',\n",
      "                                                                                         strip_accents='ascii',\n",
      "                                                                                         sublinear_tf=True),\n",
      "                                                                         'brand'),\n",
      "                                                                        ('name',\n",
      "                                                                         TfidfVectorizer(max_features=20000,\n",
      "                                                                                         ngram_range=(1,\n",
      "                                                                                                      2),\n",
      "                                                                                         stop_words='english',\n",
      "                                                                                         strip_accents='ascii',\n",
      "                                                                                         sublinear_tf=True),\n",
      "                                                                         'name'),\n",
      "                                                                        ('descr...\n",
      "                                                                                         stop_words='english',\n",
      "                                                                                         strip_accents='ascii',\n",
      "                                                                                         sublinear_tf=True),\n",
      "                                                                         'keywords')])),\n",
      "                                       ('dims', 'passthrough'),\n",
      "                                       ('clf',\n",
      "                                        RandomForestClassifier(random_state=30))]),\n",
      "             n_jobs=4,\n",
      "             param_grid=[{'dims': [TruncatedSVD(), LatentDirichletAllocation()],\n",
      "                          'dims__n_components': [2, 10, 100]},\n",
      "                         {'dims': [SelectKBest(k='all',\n",
      "                                               score_func=<function chi2 at 0x00000254CF33A8B0>)],\n",
      "                          'dims__k': [10, 7000, 'all']}])\n",
      "> 5920 samples in training set.\n",
      "Finished training pipeline in 160.252 second(s).\n",
      "Making predictions with test set: \n",
      "> 23684 samples in test set.\n",
      "Finished making predictions in 6.142 second(s).\n",
      "Accuracy Score: 62.971%\n",
      "Misclassification Score: 37.029%\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    (25, 50]       0.64      0.97      0.77     13337\n",
      "     (0, 25]       0.70      0.34      0.46      2623\n",
      " (100, 100+]       0.52      0.14      0.22      4991\n",
      "   (50, 100]       0.53      0.14      0.22      2733\n",
      "\n",
      "    accuracy                           0.63     23684\n",
      "   macro avg       0.59      0.40      0.42     23684\n",
      "weighted avg       0.61      0.63      0.56     23684\n",
      "\n",
      "Confusion matrix: \n",
      "[[12946    69   268    54]\n",
      " [ 1437   887   150   149]\n",
      " [ 4033   118   697   143]\n",
      " [ 1913   198   238   384]]\n"
     ]
    }
   ],
   "source": [
    "# Import utilities.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare the Grid parameters.\n",
    "N_FEATURES = [2, 10, 100]\n",
    "def get_param_grid():\n",
    "    \"\"\"Get the parameter grid.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"dims\": [TruncatedSVD(), LatentDirichletAllocation()],\n",
    "            \"dims__n_components\": N_FEATURES,\n",
    "        },\n",
    "        {\n",
    "            \"dims\": [SelectKBest(chi2, k='all')],\n",
    "            \"dims__k\": [10, 7000, 'all'],\n",
    "        }\n",
    "    ]\n",
    "reducer_labels = ['TruncatedSVD', 'LDA', 'KBest(chi2)']\n",
    "\n",
    "# Create the grid and run it.\n",
    "clf_RF = RandomForestClassifier(random_state=30)\n",
    "grid_RF = GridSearchCV(get_pipeline(clf_RF, \"passthrough\"), n_jobs=4, param_grid=get_param_grid())\n",
    "benchmark(\"Random Forest Classifier (Grid Search)\", grid_RF, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6868ef0-7b53-4b51-8383-b282e60bc22b",
   "metadata": {},
   "source": [
    "### Best Outcomes\n",
    "\n",
    "After using GridSearch, it seems that the best pipeline outcome with `RandomForest` might be with `SelectKBest` when selecting all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a86ae257-813b-4e41-9864-71092f133f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dims': SelectKBest(k='all', score_func=<function chi2 at 0x00000254CF33A8B0>),\n",
       " 'dims__k': 'all'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_RF.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
